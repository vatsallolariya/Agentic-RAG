# ğŸ§  Agentic RAG

A lightweight Agentic Retrieval-Augmented Generation (RAG) system built with LangGraph, LangChain, FAISS, and Azure OpenAI that intelligently decides whether retrieval is required before answering a question.

## ğŸš€ Overview

This project demonstrates how to:
1. Build an agent-like workflow using LangGraph
2. Use a Router LLM to decide if context retrieval is needed
3. Retrieve relevant chunks using FAISS vector search
4. Use Azure OpenAI GPT models to generate accurate, context-aware answers
5. Maintain a clean, modular code structure for real-world applications

ğŸ§© **Tech Stack**
- ğŸ Python 3.10+
- ğŸ”— LangChain for document + LLM orchestration
- ğŸ”€ LangGraph for multi-step state-based workflows
- âš¡ FAISS for vector similarity search
- ğŸ§¬ Azure OpenAI Embeddings
- ğŸ¤– Azure OpenAI GPT for natural language generation

## ğŸ“‚ Project Structure
```bash
agentic_rag/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ llms/
â”‚   â”œâ”€â”€ router_llm.py
â”‚   â””â”€â”€ rag_llm.py
â”‚
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ embeddings.py
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ store.py
â”‚
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ nodes.py
â”‚   â””â”€â”€ workflow.py
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

## ğŸ›  Installation
### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/vatsallolariya/Agentic-RAG.git
cd agentic_rag
```

### 2ï¸âƒ£ Create and activate a virtual environment
```bash
python -m venv .venv
.venv\Scripts\activate     # Windows
# or
source .venv/bin/activate  # Mac/Linux
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
Or using uv:
```bash
uv init
uv add -r requirements.txt
```

## ğŸ” Azure OpenAI Configuration
Create a .env file in the root directory:
```bash
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_CHAT_DEPLOYMENT==
AZURE_OPENAI_EMBED_DEPLOYMENT=
AZURE_OPENAI_API_VERSION=
```
Your credentials will be automatically loaded through python-dotenv.

## â–¶ï¸ Usage
Run the main program:
```bash
python main.py
```

Example:
```bash
print(ask_question("Explain LangGraph."))
```

## ğŸ§© LangGraph Workflow Diagram
<img width="143" height="432" alt="graph" src="https://github.com/user-attachments/assets/8cfb6c74-9906-4f19-94e9-4c2a2589c3f3" />

## âœ… Example Queries (YES & NO Scenarios)
âœ” 1. YES â€” Retrieval Needed
```bash
Query
"What is LangGraph and how does it work?"

Why YES?
Because the answer requires factual information stored in your FAISS vector database.

Router Output
YES

Flow
Retrieve documents â†’ Generate answer using context
```

âŒ 2. NO â€” Retrieval Not Needed
```bash
Query:
"Tell me a joke."

Why NO?
The question does not require document knowledge â€” the LLM already knows how to answer.

Router Output:
NO

Flow:
Skip retrieval â†’ Generate direct answer
```

